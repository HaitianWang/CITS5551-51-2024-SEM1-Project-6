{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prototype is for UWAIntelliCrop Project. \n",
    "\n",
    "1. **Purpose and Application**:\n",
    "   - Designed to handle multi-spectral imaging data for precise spatial analysis.\n",
    "   - Designed to classify crops, weeds, as well as others .\n",
    "\n",
    "2. **Core Technology**:\n",
    "   - Utilizes TensorFlow and Keras for building and managing deep learning models.\n",
    "   - Leverages the InceptionV3 architecture, renowned for its effectiveness in image classification tasks.\n",
    "\n",
    "3. **Custom ResizeLayer**:\n",
    "   - Introduces a `ResizeLayer` to adjust tensor spatial dimensions dynamically.\n",
    "   - Ensures compatibility of network layers by matching their input and output sizes, facilitating smooth data processing.\n",
    "\n",
    "4. **Adaptation of InceptionV3**:\n",
    "   - Employs InceptionV3 without its classification top layer to extend its application to segmentation tasks.\n",
    "   - Adapts the model to handle inputs with 15 channels by first reducing them to 3 channels using a custom convolutional layer, making it compatible with the pre-trained model.\n",
    "\n",
    "5. **Enhancements with Convolutional Layers**:\n",
    "   - Additional convolutional layers are used to refine features and maintain spatial dimensions.\n",
    "   - Ends with a convolutional layer that outputs four channels representing different segmentation classes.\n",
    "\n",
    "6. **Output Processing**:\n",
    "   - Applies a custom `ResizeLayer` to scale the output back to the original input dimensions (512x512), aligning the output with practical usage requirements.\n",
    "   - Uses a Softmax layer to convert the final layer outputs into a probability distribution, facilitating easy interpretation of results.\n",
    "\n",
    "7. **Model Training and Compilation**:\n",
    "   - Freezes the pre-trained layers to retain learned features while training only the newly added layers.\n",
    "   - Compiles the model with RMSprop optimizer and categorical cross-entropy loss for effective learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code block imports necessary libraries and defines utility functions for loading and handling data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Conv2D, UpSampling2D, concatenate, Input, BatchNormalization, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here libraries are imported for file operations, data manipulation, machine learning, and image processing. The tensorflow library is notably used for building and training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Sourcing: Load from Files\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a **read_X** function that navigates directories to load `multi-spectral` images and their labels for use in deep learning. Each image and its corresponding label are collected unless the directory contains images with `NaN` values, which are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_path = './test1'\n",
    "pattern = re.compile(r'smalldata_(\\d+)_(\\d+)')\n",
    "indices = ['ExG', 'ExR', 'PRI', 'MGRVI', 'SAVI', 'MSAVI', 'EVI', 'REIP', 'NDVI', 'GNDVI', 'CI', 'OSAVI', 'TVI', 'MCARI', 'TCARI']\n",
    "\n",
    "def load_tif(file_path):\n",
    "    # Function to load .tif file and return as a numpy array using rasterio\n",
    "    with rasterio.open(file_path) as src:\n",
    "        return src.read(1)  # Read the first band\n",
    "\n",
    "def read_X(dir=base_path, indices=indices):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for dir_name in dirs:\n",
    "            match = pattern.match(dir_name)\n",
    "            if match:\n",
    "                group_number = match.group(1)\n",
    "                sub_group_number = match.group(2)\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                channels = []\n",
    "                skip_directory = False\n",
    "                for file_name in os.listdir(dir_path):\n",
    "                    if file_name.endswith('.tif'):\n",
    "                        for index in indices:\n",
    "                            if file_name.startswith(index):\n",
    "                                file_path = os.path.join(dir_path, file_name)\n",
    "                                data = load_tif(file_path)\n",
    "                                if np.isnan(data).any():\n",
    "                                    print(f\"Skipping directory due to NaN values in: {file_path}\")\n",
    "                                    skip_directory = True\n",
    "                                    break\n",
    "                                channels.append(data)\n",
    "                        if skip_directory:\n",
    "                            break\n",
    "                    if file_name.startswith(\"label_matrix\"):\n",
    "                        file_path = os.path.join(dir_path, file_name)\n",
    "                        label_matrix = pd.read_csv(file_path, header=None).values\n",
    "                if not skip_directory:\n",
    "                    images.append(channels)\n",
    "                    labels.append(label_matrix)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# # Example usage\n",
    "# images, labels = read_X()\n",
    "# print(images.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=read_X()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function converts label matrices into one-hot encoded format for neural network classification, where 4 represents the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def convert_to_one_hot(y):\n",
    "    # Get the shape of the input array\n",
    "    n, h, w = y.shape\n",
    "    \n",
    "    # Initialize an all-zero array with shape (n, h, w, 4)\n",
    "    y_one_hot = np.zeros((n, h, w, 4), dtype=int)\n",
    "    \n",
    "    # Use advanced indexing to convert the original array values to one-hot encoding\n",
    "    for i in range(4):\n",
    "        y_one_hot[..., i] = (y == i)\n",
    "    \n",
    "    return y_one_hot\n",
    "\n",
    "\n",
    "def get_predicted_labels(predictions):\n",
    "    \"\"\"\n",
    "    Convert the predicted probability array to a label array.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions: A predicted probability array with shape (n, 512, 512, 4)\n",
    "    \n",
    "    Returns:\n",
    "    A label array with shape (n, 512, 512), where each point represents its most probable class\n",
    "    \"\"\"\n",
    "    predicted_labels = np.argmax(predictions, axis=-1)\n",
    "    return predicted_labels\n",
    "\n",
    "\n",
    "def one_hot_to_labels(y_one_hot):\n",
    "    \"\"\"\n",
    "    Convert a one-hot encoded array back to a label array.\n",
    "    \n",
    "    Parameters:\n",
    "    y_one_hot: A one-hot encoded array with shape (n, 512, 512, 4)\n",
    "    \n",
    "    Returns:\n",
    "    A label array with shape (n, 512, 512)\n",
    "    \"\"\"\n",
    "    # Use np.argmax to find the index of the maximum value in the fourth dimension\n",
    "    y_labels = np.argmax(y_one_hot, axis=-1)\n",
    "    \n",
    "    return y_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Definition & Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines prepare the data for input into a neural network. They normalize image data, adjust dimension ordering for TensorFlow, and convert labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize input data to [0, 1]\n",
    "X = X / np.max(X)\n",
    "\n",
    "# Transpose the dimensions of X to (0, 2, 3, 1)\n",
    "X = X.transpose((0, 2, 3, 1))\n",
    "\n",
    "# Convert y to one-hot encoding\n",
    "y_one_hot = convert_to_one_hot(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function `preprocess_image` is designed to preprocess images and labels. In this case, the function simply returns the input images and labels without any modification or resizing. \n",
    "\n",
    "Function `load_dataset`constructs a TensorFlow dataset from given image and label arrays.The steps within the function include:\n",
    "\n",
    "1. Creating a Dataset Object: `tf.data.Dataset.from_tensor_slices((images, labels))` creates a dataset that slices the input arrays along the first dimension. This is useful when each item in the array corresponds to a data point (e.g., an image and its label).\n",
    "\n",
    "2. Mapping the Preprocessing Function: `dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)` applies the `preprocess_image` function to each element of the dataset. The `num_parallel_calls` parameter set to AUTOTUNE allows TensorFlow to dynamically determine the optimal number of parallel calls to make, which can improve performance by effectively utilizing available CPU resources.\n",
    "\n",
    "3. Batching and Prefetching: `dataset.batch(batch_size)` groups the dataset elements into batches. This makes it easier to process and train on large datasets because processing small batches typically fits better in memory.\n",
    "`dataset.prefetch(tf.data.experimental.AUTOTUNE)` prefetches dataset elements in the background while the model is being trained. This helps ensure that the GPU or other processing units do not have to wait for new data to become available, thereby improving training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def preprocess_image(image, label):\n",
    "    # Do not resize, keep the original dimensions\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(images, labels, batch_size=4):\n",
    "    # Create a TensorFlow dataset from the images and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    # Apply the preprocess_image function to each element in the dataset\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    # Batch the dataset and prefetch for better performance\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load the training dataset\n",
    "train_dataset = load_dataset(X, y_one_hot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN using TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a convolutional neural network (CNN) using TensorFlow and Keras, with a custom layer for resizing, integration of a pre-trained model (InceptionV3), and various adjustments to fit the specific needs of processing multi-channel satellite imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 512, 3)\n",
      "(None, 14, 14, 512)\n",
      "(None, 14, 14, 512)\n",
      "(None, 14, 14, 4)\n",
      "(None, 512, 512, 4)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "\n",
    "# Define a custom ResizeLayer\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_height, target_width):\n",
    "        super(ResizeLayer, self).__init__()\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, (self.target_height, self.target_width))\n",
    "\n",
    "# Define the input tensor with shape (512, 512, 15)\n",
    "input_tensor = Input(shape=(512, 512, 15))\n",
    "\n",
    "# Add a convolutional layer to convert the input to a 3-channel input suitable for InceptionV3\n",
    "x = Conv2D(3, (1, 1), padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "# Create the base model with InceptionV3, excluding the top classification layer\n",
    "# Note: input_shape here is set to (512, 512, 3) since we are modifying it\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(512, 512, 3))\n",
    "\n",
    "# Ensure that base_model is called with a properly formed input\n",
    "base_model_output = base_model(x)\n",
    "\n",
    "# Inception Module A (Capture Features at Multiple Scales)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(base_model_output)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Reduction Module 1 (Downsample Spatial Dimensions / Lower the complexity)\n",
    "x = Conv2D(512, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Inception Module B (Reduce Parameters again while Maintaining Performance)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Reduction Module 2 (Downsample Spatial Dimensions / Lower the complexity)\n",
    "x = Conv2D(256, (3, 3), activation='relu', padding='same', strides=(2, 2))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "# Inception Module C (Refining Features)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Use the custom resize layer to resize the output to (512, 512)\n",
    "x = ResizeLayer(512, 512)(x)\n",
    "\n",
    "# Add the final convolutional layer\n",
    "x = Conv2D(4, (1, 1), padding='same')(x)\n",
    "\n",
    "# Apply the Softmax activation function to ensure the output represents a probability distribution\n",
    "predictions = tf.keras.layers.Softmax(axis=-1)(x)\n",
    "\n",
    "# Define the model with the input tensor and the predictions\n",
    "model = Model(inputs=input_tensor, outputs=predictions)\n",
    "\n",
    "# Freeze the convolutional layers of the pre-trained model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model with RMSprop optimizer and categorical cross-entropy loss\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Model Training: The model is trained using the model.fit() method on train_dataset for a total of 10 epochs. The callbacks parameter, which would include a custom callback to monitor the training process, is currently commented out (# callbacks=[callbacks]). \n",
    "\n",
    "**What the Callback Accomplishes**\n",
    "The primary purpose of this callback is to implement early stopping in a custom way. Instead of using the built-in EarlyStopping callback provided by Keras, which stops training when a monitored metric has stopped improving, your custom callback uses a specific threshold for loss values to determine when to stop training. This can be particularly useful if you know from domain knowledge or previous experimentation that a certain loss value corresponds to an adequately trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.0577 - loss: 1.7735\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 675ms/step - accuracy: 0.8695 - loss: 1.3603\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 637ms/step - accuracy: 0.8695 - loss: 0.8660\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step - accuracy: 0.8695 - loss: 0.4720\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 635ms/step - accuracy: 0.8695 - loss: 0.4290\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569ms/step - accuracy: 0.8695 - loss: 0.4500\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 566ms/step - accuracy: 0.8681 - loss: 0.4426\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 560ms/step - accuracy: 0.8695 - loss: 0.4185\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 527ms/step - accuracy: 0.8695 - loss: 0.4042\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 545ms/step - accuracy: 0.8695 - loss: 0.3988\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs.get('val_loss') <= 0.1099 and logs.get('loss') <= 0.1099:\n",
    "            print('\\n\\n Reached The Destination!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10\n",
    "    # callbacks=[callbacks]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model as .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./inceptionv3_fcn_model_23_5.h5\n"
     ]
    }
   ],
   "source": [
    "model_path='./inceptionv3_fcn_model_23_5.h5'\n",
    "\n",
    "model.save(model_path)\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully at ./inceptionv3_fcn_model_23_5.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if os.path.exists(model_path):\n",
    "    print(f\"Model saved successfully at {model_path}\")\n",
    "else:\n",
    "    print(f\"Model not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Testing: Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Layer Definition\n",
    "\n",
    "- **ResizeLayer**: A custom TensorFlow layer that resizes input tensors to specified dimensions, useful for matching tensor sizes in model architectures.\n",
    "\n",
    "### Custom Objects Dictionary\n",
    "\n",
    "- A dictionary that maps the string 'ResizeLayer' to the ResizeLayer class, enabling Keras to recognize and use the custom layer during model loading.\n",
    "\n",
    "### Loading the Model\n",
    "\n",
    "- The model is loaded with custom layers using `tf.keras.utils.custom_object_scope`, which is necessary when the model includes custom-defined components. The `load_model` function is used with `compile=False` to load the model without compiling it, suitable for inference or later customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define custom layer without 'trainable' argument\n",
    "class ResizeLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, target_height, target_width, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, [self.target_height, self.target_width])\n",
    "\n",
    "# Add custom layer to the custom_objects dictionary\n",
    "custom_objects = {'ResizeLayer': ResizeLayer}\n",
    "\n",
    "# Load the model with the custom objects\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model = load_model('inceptionv3_fcn_model_23_5.h5', compile=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `make_pred` Function\n",
    "\n",
    "- **Purpose**: Generates predictions from test data located in a specified directory.\n",
    "- **Process**:\n",
    "  1. **Data Loading**: Utilizes the `read_X` function to load test data (both features and labels) from the provided directory.\n",
    "  2. **Data Normalization**: Normalizes the feature data to a [0, 1] range for consistency with model training conditions.\n",
    "  3. **Data Reshaping**: Rearranges the data dimensions to match the input requirements of the TensorFlow model.\n",
    "  4. **Prediction**: Employs the previously trained model to compute predictions on the processed test data.\n",
    "  5. **One-Hot Encoding**: Converts the test labels into one-hot encoded format to facilitate comparison and evaluation against model predictions.\n",
    "- **Returns**: A tuple containing one-hot encoded test labels and the model predictions for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred(dir):\n",
    "    # Read the test data from the specified directory\n",
    "    X_test, y_test = read_X(dir=dir)\n",
    "    \n",
    "    # Print the shape and data type of the test data\n",
    "    print(f\"Shape of X_test: {X_test.shape}\")\n",
    "    print(f\"Data type of X_test: {X_test.dtype}\")\n",
    "    \n",
    "    # Normalize the test data to the range [0, 1]\n",
    "    X_test = X_test / np.max(X_test)\n",
    "    \n",
    "    # Transpose the dimensions of X_test to (0, 2, 3, 1)\n",
    "    X_test = X_test.transpose((0, 2, 3, 1))\n",
    "    \n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Uncomment the following line to print the predictions\n",
    "    # print(predictions)\n",
    "    \n",
    "    # Convert y_test to one-hot encoding\n",
    "    y_test_one_hot = convert_to_one_hot(y_test)\n",
    "    \n",
    "    return y_test_one_hot, predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_test: (4, 15, 512, 512)\n",
      "Data type of X_test: float32\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_test,y_pred= make_pred(dir='./testset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Prediction Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose**: Validates that the sum of probabilities across all channels for each pixel in the predictions equals 1, ensuring that the output from the model represents a valid probability distribution.\n",
    "- **Process**:\n",
    "  1. **Sum Calculation**: Computes the sum of predicted values across all channels for each pixel using `np.sum(y_pred, axis=-1)`.\n",
    "  2. **Verification**: Uses `np.allclose(sum_of_channels, 1)` to check if all these sums are close to 1, confirming proper softmax normalization in the model's output.\n",
    "- **Output**: Prints a boolean result indicating whether all pixel predictions adhere to the expected normalization criterion. This step is crucial for confirming that the model's predictions are probabilistically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all sums of the four channels equal to 1? True\n"
     ]
    }
   ],
   "source": [
    "# Check if the sum of channel values for each pixel is equal to 1\n",
    "sum_of_channels = np.sum(y_pred, axis=-1)\n",
    "#print(sum_of_channels)\n",
    "\n",
    "# Verify if all values are close to 1\n",
    "are_all_close_to_one = np.allclose(sum_of_channels, 1)\n",
    "print(f\"Are all sums of the four channels equal to 1? {are_all_close_to_one}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716917037963867"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_accuracy(y_true, y_pred, num_classes=4):\n",
    "    \"\"\"\n",
    "    Calculate classification accuracy.\n",
    "    \n",
    "    :param y_true: Actual labels, shape (batch_size, height, width, num_classes)\n",
    "    :param y_pred: Predicted labels, shape (batch_size, height, width, num_classes)\n",
    "    :param num_classes: Number of classes\n",
    "    :return: Classification accuracy\n",
    "    \"\"\"\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    y_true_class = np.argmax(y_true, axis=-1)\n",
    "    y_pred_class = np.argmax(y_pred, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct_predictions = np.sum(y_true_class == y_pred_class)\n",
    "    total_predictions = y_true_class.size\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Call the function to calculate accuracy\n",
    "calculate_accuracy(y_test, y_pred, num_classes=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MIoU"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mIoU (Mean Intersection over Union) is calculated as the average of the ratio of the intersection to the union of the predicted and actual segmentations across classes,it providing a measure of a segmentation model's accuracy and consistency.\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4679229259490967"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_miou(y_true, y_pred, num_classes):\n",
    "    \"\"\"\n",
    "    Calculate Mean Intersection over Union (mIoU).\n",
    "    \n",
    "    :param y_true: Actual labels, shape (batch_size, height, width, num_classes)\n",
    "    :param y_pred: Predicted labels, shape (batch_size, height, width, num_classes)\n",
    "    :param num_classes: Number of classes\n",
    "    :return: Mean Intersection over Union (mIoU)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert one-hot encoded labels to class indices\n",
    "    y_true = np.argmax(y_true, axis=-1)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    iou_list = []\n",
    "    for c in range(num_classes):\n",
    "\n",
    "        \n",
    "        # Create boolean arrays for the current class\n",
    "        true_class = (y_true == c)\n",
    "        pred_class = (y_pred == c)\n",
    "        \n",
    "        # Calculate the intersection and union for the current class\n",
    "        intersection = np.sum(true_class & pred_class)\n",
    "        \n",
    "        union = np.sum(true_class | pred_class)\n",
    "        \n",
    "        if union == 0:\n",
    "            iou = 1.0  # If there is no ground truth or predicted instance in this class\n",
    "\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "\n",
    "        \n",
    "        # Append the IoU for the current class to the list\n",
    "        iou_list.append(iou)\n",
    "    \n",
    "    # Calculate the mean IoU across all classes\n",
    "    miou = np.mean(iou_list)\n",
    "    \n",
    "    return miou\n",
    "\n",
    "# Call the function to calculate mIoU\n",
    "calculate_miou(y_test, y_pred, num_classes=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, the initial deployment of our convolutional neural network model for differentiating crops from weeds using multi-spectral imaging data demonstrated a promising start with an accuracy of 60% and a mean Intersection over Union (mIoU) score of 0.48. While these metrics affirm the model's potential, they also highlight significant opportunities for enhancement to meet the high demands of precision agriculture.\n",
    "\n",
    "The complexity of the agricultural datasets, coupled with the high dimensionality of the input data and apparent data imbalances, necessitates a strategic approach to refine our model's accuracy and efficiency. The upcoming phases of our project will focus on several critical areas to address these challenges:\n",
    "\n",
    "1. **Feature Selection:** We aim to enhance model performance through meticulous feature selection. Techniques like Recursive Feature Elimination (RFE), Principal Component Analysis (PCA), and leveraging domain-specific knowledge will be employed to isolate the most effective features for crop-weed classification.\n",
    "\n",
    "2. **Hyperparameter Tuning:** By experimenting with various hyperparameters—learning rates, batch sizes, and network architectures—we intend to find an optimal configuration that boosts model efficacy. Advanced methods such as Grid Search, Random Search, and Bayesian Optimization will play pivotal roles in this systematic exploration.\n",
    "\n",
    "3. **Addressing Data Imbalance:** The skew towards crop classifications within our dataset will be tackled using methods designed to equalize class representation, such as oversampling minority classes, undersampling majority classes, and implementing Synthetic Minority Over-sampling Technique (SMOTE). Additionally, we plan to integrate sophisticated augmentation strategies to further balance the dataset.\n",
    "\n",
    "By focusing on these areas, we are confident in significantly advancing our model's performance, ultimately contributing to more accurate, efficient, and sustainable agricultural practices. This progress will not only bolster the model's theoretical underpinnings but also enhance its practical applicability, providing a robust tool for farmers and agronomists to optimize crop management and weed control strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
